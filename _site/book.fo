<?xml version="1.0" encoding="UTF-8"?>
<fo:root xmlns:fo="http://www.w3.org/1999/XSL/Format" font-size="12pt">
  <fo:layout-master-set>
    <fo:simple-page-master master-name="PageMaster-Left" page-height="174.5mm" page-width="108.0mm" margin-bottom="6.35mm" margin-left="6.35mm" margin-right="6.35mm" margin-top="6.35mm">
      <fo:region-body margin="0mm 0mm 0mm 0mm"/>
      <fo:region-before region-name="Left-header" extent="10mm" display-align="after"/>
      <fo:region-after region-name="Left-footer" extent="10mm" display-align="before"/>
      <fo:region-start region-name="Left-start" extent="25mm"/>
      <fo:region-end region-name="Left-end" extent="25mm"/>
    </fo:simple-page-master>
    <fo:simple-page-master master-name="PageMaster-Right" page-height="174.5mm" page-width="108.0mm" margin-bottom="6.35mm" margin-left="6.35mm" margin-right="6.35mm" margin-top="6.35mm">
      <fo:region-body margin="0mm 0mm 0mm 0mm"/>
      <fo:region-before region-name="Right-header" extent="10mm" display-align="after"/>
      <fo:region-after region-name="Right-footer" extent="10mm" display-align="before"/>
      <fo:region-start region-name="Right-start" extent="25mm"/>
      <fo:region-end region-name="Right-end" extent="25mm"/>
    </fo:simple-page-master>
    <fo:page-sequence-master master-name="PageMaster">
      <fo:repeatable-page-master-alternatives>
        <fo:conditional-page-master-reference master-reference="PageMaster-Left" odd-or-even="even"/>
        <fo:conditional-page-master-reference master-reference="PageMaster-Right" odd-or-even="odd"/>
      </fo:repeatable-page-master-alternatives>
    </fo:page-sequence-master>
  </fo:layout-master-set>
  <fo:page-sequence master-reference="PageMaster" initial-page-number="1">
    <fo:flow flow-name="xsl-region-body">
      <fo:block>
  

  <fo:block page-break-before="always">
  
    
      <fo:inline font-weight="bold">Ash</fo:inline>Ash: We were somewhere around the waterfall, on the edge of the software lifecycle, when the tests began to take hold. I remember saying something like:

I feel a bit lightheaded. Maybe you should drive.

Suddenly, there was a terrible roar all around us, and the software was full of what looked like huge bugs, all swooping and screeching and diving around the computer, and a voice was screaming:

Holy Jesus. What are these goddamn bugs?

<fo:inline font-weight="bold">Dr. Gonzo</fo:inline>Dr. Gonzo: Did you say something?

<fo:inline font-weight="bold">Ash</fo:inline>Ash: Hm? Never mind. It’s your turn to test.

<fo:inline font-weight="bold">Ash</fo:inline>Ash: No point in mentioning these bugs, I thought. Poor bastard will see them soon enough.
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</fo:block>
  <fo:block page-break-before="always">Concepts</fo:block><fo:block page-break-before="always"><fo:block>challenging assumptions</fo:block><fo:block>Ask better questions - Listen!</fo:block><fo:block>Mule Testing - proactively testing assumptions</fo:block><fo:block>Mule Specs - automated assumption testing</fo:block></fo:block>
  Concepts
  
    
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">challenging assumptions</fo:block>
          Many bugs can be prevented by challenging assumptions. Challenging the assumptions every one holds as well as paying attention to the seemingly small ones, will yield great results.

Small things can be the most dangerous as they tend to go unnoticed, then gang up on you. It’s more common for projects to get overwhelmed by a build up of small things. The devil’s in the detail.

To find assumptions, listen to the way people speak. Assumptions can found in sentences that contain:


must, always, mandatory, required

impossible, inconceivable, never

should, ought

doesn’t make sense


Doesn’t make sense is a favourite. This planet is filled with humans who do many things that make more money than sense.

An Example!

Consider the following story:

As a customer  I want to know the average activity  So that I can compare this month’s activity against the average 

Sounds simple… but if you look a little deeper:


What do we mean by ‘know’?

What do we mean by average? Geometric, harmonic or arithmetic mean?

What activity?

What do you mean by month? How many days are in it?

Over what time span is the average calculated? Does it move?

How are the numbers rounded? How is the tie broken?


Remember, the most hidden assumptions are those you yourself hold. As a tester, you need to challenge yourself and question everything.
        </fo:block>
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">Ask better questions - Listen!</fo:block>
          Everything that follows is a result of what you see here.

Testing relies heavily on asking questions. Questions allow us to challenge assumptions, confirm what we already know and uncover the unknown. Coming up with the right questions and understanding what to do with the answers is a real skill. Consider yourself a detective or scientist, whichever you find more motivating.

Have you read/seen ‘i Robot’? In the story, Detective Spooner has to solve a murder. It starts with him questioning a hologram of the victim, Dr Lanning. The hologram is a simple program, it can only give limited responses to specific questions.

Detective Spooner collects pieces of information, assesses them and re-evaluates what he knows. He uses that to piece together the right questions, fueling the cycle until he uncovers the fundamental flaw that had made it into (mass) production.

He could have asked the hologram many mindless questions, but that may never have allowed him to reach the right one. Testing can be as simple as asking questions but they are futile if you’re not listening to the answers and constantly evaluating what you know.

Beware of robots.
        </fo:block>
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">Mule Testing - proactively testing assumptions</fo:block>
          We were building a shiny new system that relied on data from a poorly understood legacy monster. Assumptions about this data were baked into our system. These unchallenged assumptions turned out to be wrong. Our shiny new system was no longer so shiny.

The wider the belief in the assumption, the more it’s en-grained in the business, the greater the need for it to be tested.

Why trust when you can know?

An example Mule test (it’s blogging by example!)

Start with the assumption: “All products must have a category” Find a way to challenge or validate it. In this case we would run a simple query against production data:
    SELECT * FROM products WHERE category IS NULL


If the assumption holds true then rest easy. If it turns out to be false, congratulations you have just prevented a major bug. Share it with the team and update your old assumption to include the new facts. In this example “A product does not require a category”.

Mule testing has limits. It only helps you test assumptions that you know about. If the magic combination of data that breaks your assumptions doesn’t yet exist, it won’t fail. It only works with access to the latest production data.

Why the name mule testing? Because some people got the wrong idea when we called it ass testing.
        </fo:block>
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">Mule Specs - automated assumption testing</fo:block>
          In our last post we talked about mule testing. Assumptions need automation because they’re the foundation our systems are built upon; they can change at anytime. Mule specs are a way to automate mule tests.

You can use any automated testing tool - the one your project already uses is probably fine. Unless it’s QTP. Below is the example from the last post in RSpec using the sequel gem.
describe 'products' do
  it 'do not require a category' do
    sql = &lt;&lt;-SQL
      SELECT count(1) as row_count
      FROM product
      WHERE category IS NULL
    SQL

    at_least_one_row_exists sql
  end
end


We use two helper functions as we phrase our tests to expect either at least one result or no results.
def at_least_one_row_exists sql
  DB[sql][:row_count].should != 0
end

def no_rows_exists sql
  DB[sql][:row_count].should == 0
end


Getting production data

Mule tests require prod data, the older and less realistic it is, the less certainty you have in your assumptions. Running Mule Specs on production data doesn’t mean running them on production, that’s a really bad idea. Copy the data elsewhere before execution. We arranged a sync from production every night and our Mule Specs run against it. So, when we arrive in the morning we know that as of yesterday, all our assumptions are still true.

Mule specs give us more than just a way of verifying assumptions. Written well, with good reporting, you produce verified documentation that’s updated every night when the mules run. Get the entire team involved. Ensure the analysts note their assumptions as they go and have the testers and developers implement them.

Follow your heart, run with the mules every night.
        </fo:block>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  <fo:block page-break-before="always">Automation</fo:block><fo:block page-break-before="always"><fo:block>when should we be doing automated testing?</fo:block><fo:block>Disposable Automation</fo:block><fo:block>the limits of automation</fo:block></fo:block>
  Automation
  
    
      
    
      
    
      
    
      
    
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">when should we be doing automated testing?</fo:block>
          Automated tests, that are written <fo:inline font-weight="bold">before</fo:inline>before the code; capture the intention of the code, inform design decisions, provide rapid feedback and let us know when we are done. All of this gets us thinking about testing and ensuring that our code can be automated.

One view of test automation is to write it <fo:inline font-weight="bold">after</fo:inline>after the system code has been written so the automation has to cope with less change. We’ve found this view doesn’t hold up in practice, firstly we spend a lot of time reverse engineering the code to automate it. Secondly, if the code is changing then this is when we need test automation the most to provide us with a safety net.

Automated tests that are written <fo:inline font-weight="bold">after</fo:inline>after the code do not directly inform the design nor do they provide rapid feedback. When writing automated tests in this way we need to ask ourselves; why are we taking this approach?

If we are doing it to provide test coverage or run in the CI build then we are coming to the party late. Without visibility into what automation already exists we could be duplicating test effort. If these tests will help us build a better product then they should be written <fo:inline font-weight="bold">before</fo:inline>before the code.

If we are using automation to do exploratory testing and we intend to throw the automation code away afterwards then we can write the tests <fo:inline font-weight="bold">after</fo:inline>after. Not all automation needs to be kept it just has to help us explore.
        </fo:block>
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">Disposable Automation</fo:block>
          In our experience, testers have an unhealthy attachment to automated tests. We’re going to talk about times when throwaway automation is really helpful.

Record and playback

Sick and tired of clicking through page after page to find what you want to test? Record your path, run it, and test what actually matters. Record and playback is quick and easy. The code it creates will make your eyes bleed which doesn’t matter as long you dump it as soon as you’re done with it.

Exploratory Automation

Sometimes you need to test things that can’t easily be done manually. We were exploring a bug lurking deep within a server and found ourselves manually crafting HTTP headers in telnet. We realised it’s a lot easier to do this in code. So we did. We found the bug and threw the automation away.

Permutations and Combinations

There’s an adage that you can’t test everything. Sometimes, your tester senses tell you to cover a part of the system thoroughly. This can be done with a script that generates the various combinations. Run it over night and don’t leave your number.

Automation you decide to keep, you decide to maintain and “The things you own, end up owning you.” Tyler Durden
        </fo:block>
      
    
      
        <fo:block page-break-before="left">
          <fo:block font-weight="bold">the limits of automation</fo:block>
          Automation testing is frequently evangelised as the cure-all of software quality woes. However automated testing has limits on its effectiveness. Understanding these limits will keep us from trying to automate something that should never be.

Scoping Limitations

In an automated test, deviations from the norm are not necessarily reported as failures. We can work around that by writing more tests, each of them focusing on one factor of the system.

<fo:inline font-weight="bold">Practical Limitations</fo:inline>Practical Limitations: automation comes with a maintenance cost as the product evolves. This places practicality limits around what we automate. It’s not feasible to automate everything, as we must maintain everything. We need to be prudent about what tests we want to keep.

<fo:inline font-weight="bold">Technological Limitations</fo:inline>Technological Limitations: some testing activities are just not possible to automate, like user experience testing. As soon as we move into the area where subjective qualities are being measured automation breaks down.

<fo:inline font-weight="bold">Usefulness Limitations</fo:inline>Usefulness Limitations: automated tests do not provide equal value to the team. The high use post-deploy smoke test is more valuable than checking whether the user name field supports “Travis” as well as “Cornelius”. Just like we risk and value assess our manual testing effort we should be doing the same with automation.

Conflicting Limitations

The limitations we touched on they fall into two categories; those that force us to be smarter about the small set of tests we automate and those that drive us to want more tests. We can’t have both. How we deal with this is what makes a good tester.
        </fo:block>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  <fo:block page-break-before="always">Guidance</fo:block><fo:block page-break-before="always"/>
  Guidance
  
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          testing large datasets - an introduction
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          testing large datasets - an introduction
          require 'cromulent_testing'

describe 'automated testing,' do
  describe 'to involve the whole team' do
    it 'should make it easy for the team to contribute'
    it 'should include everyone in defining specifications'
    it 'should support the developers'
    it 'should be in the same language as the code'
  end

  describe 'to be an asset' do
    it 'should make it easier for the product to change'
    it 'should drive the domain model'
    it 'should generate human understandable documentation'
    it 'should cover the boring, repetative work'
    it 'should cover the high risk areas'
  end

  describe 'to be maintenable' do
    it 'should be organised in a way that makes sense'
    it 'should be easy to see why it failed'
    it 'should be discrete'
    it 'should be written as though it were production code'
  end

  describe 'to do it poorly' do
    it 'should be done after the manual testing'
    it 'should be done by a separate team'
    it 'should be done by people who don\'t understand testing'
    it 'should be implemented by people who are not skilled in coding'
    it 'should summon a balrog!'
  end
end


        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  <fo:block page-break-before="always">People</fo:block><fo:block page-break-before="always"/>
  People
  
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          Do you know about test fatigue?
        
      
        
          Dealing with test fatigue
        
      
        
      
        
      
        
      
        
      
        
      
    
  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          Do you know about test fatigue?
          
Fatigue is a normal result of working, mental stress, overstimulation and understimulation, jet lag or active recreation, depression, and also boredom, disease and lack of sleep.[1]


We could rewrite the above quote to be:

Test fatigue is a normal result of testing, delivery pressures, thrashing, uninteresting work, disenfranchisement, mechanical work, bad practices and working overtime.

What’s wrong with that?


…mental fatigue, in turn, can manifest itself both as somnolence (decreased wakefulness), or just as a general decrease of attention, not necessarily including sleepiness. Decreased attention is known as ego depletion and occurs when the limited ‘self regulatory capacity’ is depleted. It may also be described as a more or less decreased level of consciousness. In any case, this can be dangerous when performing tasks that require constant concentration, such as driving a vehicle… [or testing][1]


This is a big topic, we have a lot more to say…stay tuned.

[1]: http://en.wikipedia.org/wiki/Fatigue_(medical)
        
      
    
      
        
          Dealing with test fatigue
          Here are the problems we raised in our last post and ways we deal with them.

<fo:inline font-weight="bold">working overtime</fo:inline>working overtime - You can’t test tired. If you’re going to be working overtime for several hours, have a break. Take time away from the project and go out for dinner, like a second lunch. Adjust the workplace to your style, watch YouTube together and take frequent communal breaks.

<fo:inline font-weight="bold">delivery pressures</fo:inline>delivery pressures - The more pressure the team is under, the more likely they are to make mistakes and the more you need to test. DON’T PANIC. The less time you have the more you need to get it right the first time.

<fo:inline font-weight="bold">thrashing</fo:inline>thrashing - Make a task list of what needs doing and divvy up the work. Stop people from interrupting (think: cone of silence) by politely explaining the urgency of what you’re working on. Remember, prioritisation! It’s normally better to finish some things than to partially complete lots of them.

<fo:inline font-weight="bold">uninteresting work</fo:inline>uninteresting work - Spice up boring work by trying it in a new way. Any technique will do, invent your own or try something from your favourite testing blog. You can make work fun.

<fo:inline font-weight="bold">mechanical work</fo:inline>mechanical work - Automate it, computers love repetitive tasks. Even if you dispose of it later, you’re saving time. Delegate it to the development team, they love repetitive tasks.

<fo:inline font-weight="bold">bad practices &amp; disenfranchisement</fo:inline>bad practices &amp; disenfranchisement - Why are you doing this to yourself? Good testers are a rare breed. Other companies want you, we want you. If you can’t fix it, leave.
        
      
    
      
    
      
    
      
    
      
    
      
    
  

  <fo:block page-break-before="always">Stories</fo:block><fo:block page-break-before="always"/>
  Stories
  
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          Chicken Little
        
      
        
      
    
  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          Chicken Little
          Once upon a time there was a tiny chicken [tester] named Chicken Little. One day Chicken Little was scratching in the garden when an acorn fell on her head. “Oh,” cried Chicken Little, “The sky is falling! I must go tell the king [project manager].”

It’s important to remember that this is a tester who really cares, we need to harness their passion. A panicked approach causes stress and real problems get lost in the noise. The tester will lose credibility, become marginalised and burnout.

Harness the passion!

We need to work closely with these testers who are emotionally invested and vulnerable to criticism. How they arrived at this behaviour is irrelevant. Two things we’ve found that help are to teach them prioritisation and to value quality over quantity.

Teach them to prioritise

Ask them to rank bugs in the order they would like them fixed. If they struggle, begin by ranking one critical and one trivial bug. This forces them to understand some bugs are more important than others. Once they’re all ranked, discuss at which point we could release with the remaining bugs.

Value them

Publicly acknowledge them for finding the good bugs. Let them see their good bugs being fixed. Recognise their less important bugs and use their prioritisation to explain why they won’t be fixed.

Teach them how to find the important bugs … coming soon

…and they all lived happily ever after.
        
      
    
      
    
  

</fo:block>
    </fo:flow>
  </fo:page-sequence>
</fo:root>
